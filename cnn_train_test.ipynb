{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "from keras import layers\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import TensorBoard\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================================================\n",
      "训练集中第一条数据：\n",
      "url: POST /tienda1/publico/pagar.jsp?precio=85&B1=Pasar%2Bpor%2Bcaja%27%2C%270%27%2C%270%27%29%3Bwaitfor+delay+%270%3A0%3A15%27%3B--\n",
      "label: SQLi\n",
      "\n",
      "测试集中第一条数据：\n",
      "url: GET /tienda1/publico/caracteristicas.jsp?id=d%27z%220\n",
      "label: SQLi\n",
      "\n",
      "=================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# 读取train.csv和test.csv数据\n",
    "df_train = pd.read_csv('./data/torpeda_train_test/train.csv')\n",
    "df_test = pd.read_csv('./data/torpeda_train_test/test.csv')\n",
    "\n",
    "# 获取训练集中的url和label\n",
    "url_train = df_train['url']\n",
    "label_train = df_train['label']\n",
    "# 获取测试集中的url和label\n",
    "url_test = df_test['url']\n",
    "label_test = df_test['label']\n",
    "\n",
    "print(\"=================================================================================================\")\n",
    "print(\"训练集中第一条数据：\")\n",
    "print(\"url: %s\" % url_train[0])\n",
    "print(\"label: %s\\n\" % label_train[0])\n",
    "\n",
    "print(\"测试集中第一条数据：\")\n",
    "print(\"url: %s\" % url_test[0])\n",
    "print(\"label: %s\\n\" % label_test[0])\n",
    "print(\"=================================================================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "标签类型字典：\n",
      "{'SQLi': 0, 'anomalous': 1, 'normal': 2, 'XSS': 3, 'SSI': 4, 'BufferOverflow': 5, 'CRLFi': 6, 'XPath': 7, 'LDAPi': 8, 'FormatString': 9}\n"
     ]
    }
   ],
   "source": [
    "labels_type = pd.Series(label_train).value_counts().keys().tolist()\n",
    "# 构建标签类型字典 (Bây giờ ta sử dụng 'labels_index' thay vì 'type')\n",
    "type_dict = dict([(labels_type[i], i) for i in range(len(labels_type))])\n",
    "print(\"标签类型字典：\")\n",
    "print(type_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "字典的大小为98\n",
      "字典：\n",
      "{'%': 1, '2': 2, '0': 3, 'i': 4, '3': 5, 'r': 6, '=': 7, 'e': 8, 'o': 9, 'a': 10, '&': 11, 'c': 12, 'm': 13, '1': 14, '7': 15, 'd': 16, 'n': 17, 'C': 18, '5': 19, '6': 20, 't': 21, 'p': 22, 'L': 23, 's': 24, '9': 25, '8': 26, 'l': 27, '/': 28, ',': 29, '4': 30, 'N': 31, 'b': 32, 'U': 33, 'u': 34, 'O': 35, 'g': 36, '.': 37, 'S': 38, 'T': 39, 'E': 40, 'A': 41, 'j': 42, 'P': 43, ' ': 44, 'R': 45, 'w': 46, 'B': 47, '?': 48, 'F': 49, 'D': 50, 'v': 51, 'f': 52, 'I': 53, '+': 54, '-': 55, 'x': 56, 'z': 57, 'h': 58, 'H': 59, 'M': 60, '#': 61, 'y': 62, 'G': 63, ';': 64, 'K': 65, 'Z': 66, 'J': 67, 'V': 68, 'Y': 69, 'k': 70, 'X': 71, 'W': 72, 'Q': 73, '<': 74, '>': 75, 'q': 76, '\"': 77, '_': 78, '@': 79, '*': 80, ':': 81, '(': 82, ')': 83, \"'\": 84, '!': 85, '[': 86, ']': 87, '{': 88, '}': 89, '`': 90, '\\r': 91, '$': 92, '~': 93, '|': 94, '\\\\': 95, '^': 96, '\\n': 97}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "# 字符级分词，在训练集上拟合\n",
    "tokenizer = Tokenizer(filters='\\t\\n', char_level=True)\n",
    "tokenizer.fit_on_texts(url_train)\n",
    "# 构建词典，并保存\n",
    "num_words = len(tokenizer.word_index)+1\n",
    "vocab = tokenizer.word_index\n",
    "print(\"字典的大小为%d\" % num_words)\n",
    "print(\"字典：\")\n",
    "print(vocab)\n",
    "with open(\"./tokenizer/vocab.json\", 'w') as f:\n",
    "    json.dump(vocab, f, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将label转化为one-hot形式\n",
    "def get_one_hot_value(s):\n",
    "    return [0 if i!= type_dict[s] else 1 for i in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "603.239999999998\n",
      "604.8299999999981\n"
     ]
    }
   ],
   "source": [
    "# 获取url的长度\n",
    "url_train_lens = [len(u) for u in url_train]\n",
    "url_test_lens = [len(u) for u in url_test]\n",
    "# 查看97%长度的大小\n",
    "print(np.percentile(np.array(url_train_lens),97))\n",
    "print(np.percentile(np.array(url_test_lens),97))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================================================\n",
      "举例：\n",
      "向量化前：\n",
      "url: POST /tienda1/publico/pagar.jsp?precio=85&B1=Pasar%2Bpor%2Bcaja%27%2C%270%27%2C%270%27%29%3Bwaitfor+delay+%270%3A0%3A15%27%3B--\n",
      "label: SQLi\n",
      "\n",
      "向量化后：\n",
      "url_vec: [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 43 35 38 39 44 28 21\n",
      "  4  8 17 16 10 14 28 22 34 32 27  4 12  9 28 22 10 36 10  6 37 42 24 22\n",
      " 48 22  6  8 12  4  9  7 26 19 11 47 14  7 43 10 24 10  6  1  2 47 22  9\n",
      "  6  1  2 47 12 10 42 10  1  2 15  1  2 18  1  2 15  3  1  2 15  1  2 18\n",
      "  1  2 15  3  1  2 15  1  2 25  1  5 47 46 10  4 21 52  9  6 54 16  8 27\n",
      " 10 62 54  1  2 15  3  1  5 41  3  1  5 41 14 19  1  2 15  1  5 47 55 55]\n",
      "label_one_hot: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "=================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# 观察后定义url最大的长度为600\n",
    "max_len = 600\n",
    "# 将文本转为序列\n",
    "seq_train = tokenizer.texts_to_sequences(url_train)\n",
    "seq_test = tokenizer.texts_to_sequences(url_test)\n",
    "# 将序列的长度统一为max_len\n",
    "X_train = sequence.pad_sequences(seq_train, maxlen=max_len)\n",
    "X_test = sequence.pad_sequences(seq_test, maxlen=max_len)\n",
    "# 将标签转化为one-hot\n",
    "Y_train = [get_one_hot_value(l) for l in label_train]\n",
    "Y_test =  [get_one_hot_value(l) for l in label_test]\n",
    "print(\"=================================================================================================\")\n",
    "print(\"举例：\")\n",
    "print(\"向量化前：\")\n",
    "print(\"url: %s\" % url_train[0])\n",
    "print(\"label: %s\\n\" % label_train[0])\n",
    "print(\"向量化后：\")\n",
    "print(\"url_vec: \" + str(X_train[0]))\n",
    "print(\"label_one_hot: \" + str(Y_train[0]))\n",
    "print(\"=================================================================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 回调\n",
    "tb_callback = TensorBoard(log_dir='./logs', embeddings_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 600, 64)           6272      \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 594, 32)           14368     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 118, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 112, 32)           7200      \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                330       \n",
      "=================================================================\n",
      "Total params: 28,170\n",
      "Trainable params: 28,170\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# 搭建网络\n",
    "model = Sequential()\n",
    "model.add(layers.Embedding(num_words, 64, input_length=max_len))\n",
    "model.add(layers.Conv1D(32, 7, activation='relu'))\n",
    "model.add(layers.MaxPooling1D(5))\n",
    "model.add(layers.Conv1D(32, 7, activation='relu'))\n",
    "model.add(layers.GlobalMaxPooling1D())\n",
    "model.add(layers.Dense(10, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 38919 samples, validate on 12974 samples\n",
      "Epoch 1/6\n",
      "32000/38919 [=======================>......] - ETA: 28s - loss: 0.7860 - acc: 0.7543"
     ]
    }
   ],
   "source": [
    "# 训练\n",
    "model.fit(X_train, Y_train, validation_split=0.25, epochs=6, batch_size=128, callbacks=[tb_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 评估模型\n",
    "model.evaluate(X_test, Y_test, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将softmax的概率值转化为标签名\n",
    "def props_to_labels(props_matrix):\n",
    "    labels = []\n",
    "    for props_vector in props_matrix:\n",
    "        idx = np.argmax(props_vector)\n",
    "        label = labels_type[idx]\n",
    "        labels.append(label)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test_pred = model.predict(X_test)\n",
    "label_test_pred = props_to_labels(Y_test_pred)\n",
    "print(\"混淆矩阵：\")\n",
    "print(metrics.confusion_matrix(label_test, label_test_pred, labels_type))\n",
    "print(\"f1-score:\")\n",
    "print(metrics.f1_score(label_test, label_test_pred, labels_type, average='micro'))\n",
    "print(\"acc-score:\")\n",
    "print(metrics.accuracy_score(label_test, label_test_pred, labels_type))\n",
    "print(\"recall-score:\")\n",
    "print(metrics.recall_score(label_test, label_test_pred, labels_type, average='micro'))\n",
    "print(\"classification report:\")\n",
    "print(metrics.classification_report(label_test, label_test_pred, labels_type))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# 保存模型\n",
    "model.save_weights('./model/cnn_weights_nckh.h5')\n",
    "model.save('./model/cnn_clf_nckh.h5')\n",
    "with open('./model/cnn_clf_nckh.json', 'w') as f:\n",
    "    f.write(model.to_json())\n",
    "\n",
    "# 把tokenzier保存成pickle格式\n",
    "with open('./tokenizer/tokenizer.pickle_nckh', 'wb') as handle: \n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
